{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classificador Bagging\n",
    "Nesse exercício, faremos um classificador de doenças cardíacas usando ensembles de árvores de decisão com o algoritmo Bagging.\n",
    "\n",
    "Para isso, usaremos o dataset \"Heart Disease UCI\", que já está salvo no arquivo 'heart.csv'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Primeiramente, execute a célula seguinte para carregar o dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "df = pd.read_csv('data_aula_08.csv')\n",
    "X = df.drop(['target'], 1).to_numpy()\n",
    "y = df['target'].to_numpy()\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Bootstrapping\n",
    "\n",
    "Uma parte fundamental do método Bagging é o Bootstrapping, que consiste em fazermos amostragem com reposição para criarmos variações do nosso dataset. Implemente abaixo uma função que receba um dataset nas variáveis X e y e retorne um conjunto derivado usando Bootstrapping: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bag(X, y):\n",
    "    \n",
    "    X_bootstrap = None\n",
    "    y_bootstrap = None\n",
    "    \n",
    "    ######################################################################################\n",
    "    # 1. Preencha essa função para retornar uma variação do dataset passado pelos argumentos usando bootstrapping\n",
    "    # Ao final, as variáveis X_bootstrap e y_bootstrap devem conter o novo dataset após o processo de amostragem\n",
    "    # Cuidado: é necessário realizar as mesmas operações nas variáveis X e y\n",
    "    # A função np.random.choice pode ser útil\n",
    "        \n",
    "    \n",
    "    \n",
    "    ######################################################################################\n",
    "    return X_bootstrap, y_bootstrap\n",
    "\n",
    "\n",
    "\n",
    "# Verificação de Erros\n",
    "bagged = bag(X,y)\n",
    "assert type(bagged[0]) == np.ndarray and type(bagged[1]) == np.ndarray, 'Os valores retornados devem ser arrays numpy'\n",
    "assert bagged[0].shape == X.shape and bagged[1].shape == y.shape, 'Os conjuntos retornados pela função devem ter o mesmo tamanho dos originais'\n",
    "assert len(np.unique(bagged[0], axis = 0)) < 300, 'A amostragem deve ser feita com reposição'\n",
    "assert np.all([(line == X).all(1).any() for line in bagged[0]]), 'Todos os elementos retornados devem estar presentes no dataset original'\n",
    "assert np.all(bagged[1] == [y[np.argmax((line == X).all(1))] for line in bagged[0]]), 'A amostragem deve ser feita identicamente nos conjuntos X e y'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Treinamento da Árvore de Decisão\n",
    "O método bagging consiste em treinarmos diversas árvores de decisão em datasets distintos, gerados por bootstrapping. Implemente abaixo as funções para o treinamento e avaliação de uma árvore individual em uma instância do dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "def train_tree(X, y, max_depth = 10):\n",
    "    \n",
    "    x_train, y_train = bag(X, y)\n",
    "    clf = None\n",
    "    \n",
    "    ######################################################################################\n",
    "    # 2. Treine uma árvore de decisão no conjunto gerado acima (x_train e y_train)\n",
    "    # Ao final, a variável clf deve conter a árvore de decisão treinada\n",
    "    #\n",
    "    # Se quiser fazer alterações nos hiperparâmetros das árvores, você pode configurá-los aqui\n",
    "    \n",
    "    \n",
    "    \n",
    "    ######################################################################################\n",
    "    return clf\n",
    "\n",
    "\n",
    "\n",
    "# Verificação de Erros\n",
    "assert (type(train_tree(X,y)) == DecisionTreeClassifier), 'A função deve retornar uma árvore de decisão'\n",
    "assert 'tree_' in DecisionTreeClassifier().fit(X,y).__dict__, 'A árvore retornada já deve estar treinada no conjunto de treino'\n",
    "assert np.all([train_tree(X, y, i).max_depth == i for i in range(1,10)]), 'A árvore deve ter a profundidade máxima (max_depth) passada como parâmetro para a função'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "def tree_accuracy(tree, x_test, y_test):\n",
    "    \n",
    "    acc = None\n",
    "    \n",
    "    ######################################################################################\n",
    "    # 3. Calcule a acurácia da árvore tree no conjunto de teste\n",
    "    # Ao final, a variável acc deve conter o valor da acurácia\n",
    "    \n",
    "    \n",
    "\n",
    "    ######################################################################################\n",
    "    return acc\n",
    "\n",
    "    \n",
    "# Verificação de Erros    \n",
    "assert tree_accuracy(DecisionTreeClassifier(max_depth = None).fit(X, y), X, y) == 1.0, 'Erro na função de acurácia'\n",
    "assert tree_accuracy(DecisionTreeClassifier(max_depth = 1).fit(X, y), X, y) < 1.0, 'Erro na função de acurácia'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Treinamento da Ensemble de Árvores\n",
    "\n",
    "Por fim, iremos treinar várias árvores em conjuntos de dados diferentes, usando as funções anteriores:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_ensemble(X, y, n_trees = 100):\n",
    "    trees = []\n",
    "    \n",
    "    ######################################################################################\n",
    "    # 4. Treine n_trees árvores de decisão usando a função train_tree\n",
    "    # Ao final, a variável trees deve conter as árvores treinadas\n",
    "    #\n",
    "    # Decida aqui a altura máxima das árvores, passada como argumento para a função train_tree\n",
    "    # Se quiser, você pode alterar esse valor depois\n",
    "    \n",
    "    \n",
    "    \n",
    "    ######################################################################################\n",
    "    return trees\n",
    "\n",
    "\n",
    "\n",
    "# Verificação de Erros\n",
    "trees = train_ensemble(X,y,10)\n",
    "assert type(trees) == list, 'A função deve retornar uma lista contendo as árvores treinadas'\n",
    "assert len(train_ensemble(X,y,9)) == 9 and len(train_ensemble(X,y,12)) == 12, 'A lista retornada deve ter exatamente n_trees elementos'\n",
    "assert np.all([type(tree) == DecisionTreeClassifier for tree in trees]), 'Todos os elementos da lista retornada devem ser uma árvore de decisão'\n",
    "assert np.all(['tree_' in tree.__dict__ for tree in trees]), 'As árvores de decisão retornadas devem estar treinadas'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para fazermos novas predições, devemos calcular a média dos valores preditos por cada árvore individual:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ensemble_predict(trees, x):\n",
    "    preds = None\n",
    "    ######################################################################################\n",
    "    # 5. Use a ensemble trees para fazer predições nos dados x\n",
    "    # Ao final, a variável preds deve conter as predições da ensemble\n",
    "    \n",
    "    \n",
    "    \n",
    "    ######################################################################################\n",
    "    return preds\n",
    "\n",
    "trees = train_ensemble(X,y,10)\n",
    "assert ensemble_predict(trees, X).shape == y.shape, 'A array retornada pela função deve ter o mesmo número de elementos que a variável y correspondente'\n",
    "assert np.all((ensemble_predict(trees, X))*len(trees) == np.sum([tree.predict(X) for tree in trees], 0)), 'A predição feita pela ensemble deve ser a média das predições individuais'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Por fim, implemente uma função para medir a acurácia da ensemble:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ensemble_accuracy(trees, x_test, y_test):\n",
    "    acc = None\n",
    "    \n",
    "    ######################################################################################\n",
    "    # 6. Calcule a acurácia da ensemble trees no conjunto de teste\n",
    "    # Ao final, a variável acc deve conter o valor da acurácia\n",
    "    #\n",
    "    # OBS: Lembre-se que os valores retornados pela função ensemble_predict são probabilidades entre 0 ou 1.\n",
    "    #      Por isso, pode ser necessário usar uma função como np.round para transformar essas predições em uma variável binária\n",
    "    \n",
    "    \n",
    "    \n",
    "    ######################################################################################    \n",
    "    return acc\n",
    "\n",
    "\n",
    "\n",
    "# Verificação de Erros\n",
    "assert ensemble_accuracy([DecisionTreeClassifier(max_depth = None).fit(X, y)] * 10, X, y) == 1.0, 'Erro na função de acurácia'\n",
    "assert ensemble_accuracy([DecisionTreeClassifier(max_depth = 1).fit(X, y)] * 10, X, y) < 1.0, 'Erro na função de acurácia'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. K-Fold Cross Validation\n",
    "\n",
    "Devido ao dataset pequeno, usaremos K-Fold Cross Validation em vez de um simples train test split, para tornar as estimativas de performance mais confiáveis.\n",
    "\n",
    "Preencha abaixo o código para finalizarmos o treinamento de uma ensemble de árvores de decisão:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se tudo correr bem, o resultado da ensemble será consideravelmente melhor que as árvores individuais, por 5-10%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "\n",
    "# Cria uma divisão KFold com 3 splits\n",
    "kf = KFold(n_splits = 3, shuffle = True, random_state = 2)\n",
    "\n",
    "# Variável com as acurácias do modelo em cada split\n",
    "ensemble_accs = []\n",
    "tree_accs = []\n",
    "\n",
    "for train_indices, test_indices in kf.split(X,y):\n",
    "    \n",
    "    # Define os conjuntos de treino e teste do split atual\n",
    "    x_train = X[train_indices]\n",
    "    x_test = X[test_indices]\n",
    "    y_train = y[train_indices]\n",
    "    y_test = y[test_indices]\n",
    "    \n",
    "    \n",
    "    \n",
    "    ensemble_acc = None\n",
    "    ######################################################################################\n",
    "    # 7. Treine uma ensemble nos dados de treino e a avalie nos dados de teste\n",
    "    # Ao final, a variável ensemble_acc deve conter o valor da acurácia no split atual\n",
    "    # Experimente com o número de árvores na ensemble (argumento n_trees da função train_ensemble) para conseguir os melhores resultados\n",
    "    \n",
    "    \n",
    "    \n",
    "    ######################################################################################\n",
    "    \n",
    "    ensemble_accs.append(ensemble_acc)\n",
    "    tree_accs.append(np.mean([tree_accuracy(tree, x_test, y_test) for tree in trees]))\n",
    "\n",
    "    \n",
    "print('Acurácia média das árvores de decisão individuais: %.2lf%%' % (100 * np.mean(tree_accs)))\n",
    "print('Acurácia média da ensemble inteira: %.2lf%%' % (100 * np.mean(ensemble_accs)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}