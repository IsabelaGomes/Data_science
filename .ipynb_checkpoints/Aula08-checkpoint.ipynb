{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classificador Bagging\n",
    "Nesse exercício, faremos um classificador de doenças cardíacas usando ensembles de árvores de decisão com o algoritmo Bagging.\n",
    "\n",
    "Para isso, usaremos o dataset \"Heart Disease UCI\", que já está salvo no arquivo 'heart.csv'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Primeiramente, execute a célula seguinte para carregar o dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>sex</th>\n",
       "      <th>cp</th>\n",
       "      <th>trestbps</th>\n",
       "      <th>chol</th>\n",
       "      <th>fbs</th>\n",
       "      <th>restecg</th>\n",
       "      <th>thalach</th>\n",
       "      <th>exang</th>\n",
       "      <th>oldpeak</th>\n",
       "      <th>slope</th>\n",
       "      <th>ca</th>\n",
       "      <th>thal</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>63</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>145</td>\n",
       "      <td>233</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>150</td>\n",
       "      <td>0</td>\n",
       "      <td>2.3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>37</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>130</td>\n",
       "      <td>250</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>187</td>\n",
       "      <td>0</td>\n",
       "      <td>3.5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>41</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>130</td>\n",
       "      <td>204</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>172</td>\n",
       "      <td>0</td>\n",
       "      <td>1.4</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>56</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>120</td>\n",
       "      <td>236</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>178</td>\n",
       "      <td>0</td>\n",
       "      <td>0.8</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>57</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>120</td>\n",
       "      <td>354</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>163</td>\n",
       "      <td>1</td>\n",
       "      <td>0.6</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   age  sex  cp  trestbps  chol  fbs  restecg  thalach  exang  oldpeak  slope  \\\n",
       "0   63    1   3       145   233    1        0      150      0      2.3      0   \n",
       "1   37    1   2       130   250    0        1      187      0      3.5      0   \n",
       "2   41    0   1       130   204    0        0      172      0      1.4      2   \n",
       "3   56    1   1       120   236    0        1      178      0      0.8      2   \n",
       "4   57    0   0       120   354    0        1      163      1      0.6      2   \n",
       "\n",
       "   ca  thal  target  \n",
       "0   0     1       1  \n",
       "1   0     2       1  \n",
       "2   0     2       1  \n",
       "3   0     2       1  \n",
       "4   0     2       1  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "df = pd.read_csv('data_aula_08.csv')\n",
    "X = df.drop(['target'], 1).to_numpy()\n",
    "y = df['target'].to_numpy()\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Bootstrapping\n",
    "\n",
    "Uma parte fundamental do método Bagging é o Bootstrapping, que consiste em fazermos amostragem com reposição para criarmos variações do nosso dataset. Implemente abaixo uma função que receba um dataset nas variáveis X e y e retorne um conjunto derivado usando Bootstrapping: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bag(X, y):\n",
    "    \n",
    "    X_bootstrap = None\n",
    "    y_bootstrap = None\n",
    "    \n",
    "    ######################################################################################\n",
    "    # 1. Preencha essa função para retornar uma variação do dataset passado pelos argumentos usando bootstrapping\n",
    "    # Ao final, as variáveis X_bootstrap e y_bootstrap devem conter o novo dataset após o processo de amostragem\n",
    "    # Cuidado: é necessário realizar as mesmas operações nas variáveis X e y\n",
    "    # A função np.random.choice pode ser útil\n",
    "    \n",
    "    sample = len(X)\n",
    "    index = np.arange(len(X))\n",
    "    X_l = []\n",
    "    y_l = []\n",
    "    \n",
    "    while len(X_l) < sample:\n",
    "        index_bootstrap = np.random.choice(index)\n",
    "        X_l.append(X[index_bootstrap])\n",
    "        y_l.append(y[index_bootstrap])\n",
    "    \n",
    "    X_bootstrap = np.array(X_l)\n",
    "    y_bootstrap = np.array(y_l)\n",
    "    \n",
    "    ######################################################################################\n",
    "    return X_bootstrap, y_bootstrap\n",
    "\n",
    "# Create a random subsample from the dataset with replacement\n",
    "\n",
    "# Verificação de Erros\n",
    "bagged = bag(X,y)\n",
    "assert type(bagged[0]) == np.ndarray and type(bagged[1]) == np.ndarray, 'Os valores retornados devem ser arrays numpy'\n",
    "assert bagged[0].shape == X.shape and bagged[1].shape == y.shape, 'Os conjuntos retornados pela função devem ter o mesmo tamanho dos originais'\n",
    "assert len(np.unique(bagged[0], axis = 0)) < 300, 'A amostragem deve ser feita com reposição'\n",
    "assert np.all([(line == X).all(1).any() for line in bagged[0]]), 'Todos os elementos retornados devem estar presentes no dataset original'\n",
    "assert np.all(bagged[1] == [y[np.argmax((line == X).all(1))] for line in bagged[0]]), 'A amostragem deve ser feita identicamente nos conjuntos X e y'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[63.,  0.,  0., ...,  1.,  0.,  2.],\n",
       "       [54.,  0.,  2., ...,  2.,  1.,  2.],\n",
       "       [77.,  1.,  0., ...,  2.,  3.,  2.],\n",
       "       ...,\n",
       "       [76.,  0.,  2., ...,  1.,  0.,  2.],\n",
       "       [54.,  1.,  0., ...,  1.,  1.,  3.],\n",
       "       [56.,  0.,  1., ...,  1.,  0.,  2.]])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " bagged[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Treinamento da Árvore de Decisão\n",
    "O método bagging consiste em treinarmos diversas árvores de decisão em datasets distintos, gerados por bootstrapping. Implemente abaixo as funções para o treinamento e avaliação de uma árvore individual em uma instância do dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "def train_tree(X, y, max_depth = 10):\n",
    "    \n",
    "    x_train, y_train = bag(X, y)\n",
    "    clf = None\n",
    "    \n",
    "    ######################################################################################\n",
    "    # 2. Treine uma árvore de decisão no conjunto gerado acima (x_train e y_train)\n",
    "    # Ao final, a variável clf deve conter a árvore de decisão treinada\n",
    "    #\n",
    "    # Se quiser fazer alterações nos hiperparâmetros das árvores, você pode configurá-los aqui\n",
    "    \n",
    "    clf = DecisionTreeClassifier(max_depth = max_depth)\n",
    "    clf = clf.fit(x_train, y_train)\n",
    "    \n",
    "    ######################################################################################\n",
    "    return clf\n",
    "\n",
    "# Verificação de Erros\n",
    "assert (type(train_tree(X,y)) == DecisionTreeClassifier), 'A função deve retornar uma árvore de decisão'\n",
    "assert 'tree_' in DecisionTreeClassifier().fit(X,y).__dict__, 'A árvore retornada já deve estar treinada no conjunto de treino'\n",
    "assert np.all([train_tree(X, y, i).max_depth == i for i in range(1,10)]), 'A árvore deve ter a profundidade máxima (max_depth) passada como parâmetro para a função'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "def tree_accuracy(tree, x_test, y_test):\n",
    "    \n",
    "    acc = None\n",
    "    \n",
    "    ######################################################################################\n",
    "    # 3. Calcule a acurácia da árvore tree no conjunto de teste\n",
    "    # Ao final, a variável acc deve conter o valor da acurácia\n",
    "    \n",
    "    y_pred = tree.predict(x_test)\n",
    "    acc = accuracy_score(y_test, y_pred)\n",
    "\n",
    "    ######################################################################################\n",
    "    return acc\n",
    "\n",
    "    \n",
    "# Verificação de Erros    \n",
    "assert tree_accuracy(DecisionTreeClassifier(max_depth = None).fit(X, y), X, y) == 1.0, 'Erro na função de acurácia'\n",
    "assert tree_accuracy(DecisionTreeClassifier(max_depth = 1).fit(X, y), X, y) < 1.0, 'Erro na função de acurácia'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Treinamento da Ensemble de Árvores\n",
    "\n",
    "Por fim, iremos treinar várias árvores em conjuntos de dados diferentes, usando as funções anteriores:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_ensemble(X, y, n_trees = 100):\n",
    "    trees = []\n",
    "    \n",
    "    ######################################################################################\n",
    "    # 4. Treine n_trees árvores de decisão usando a função train_tree\n",
    "    # Ao final, a variável trees deve conter as árvores treinadas\n",
    "    #\n",
    "    # Decida aqui a altura máxima das árvores, passada como argumento para a função train_tree\n",
    "    # Se quiser, você pode alterar esse valor depois\n",
    "    \n",
    "    while len(trees) < n_trees:\n",
    "        trees.append(train_tree(X, y, max_depth = 10))\n",
    "    \n",
    "    ######################################################################################\n",
    "    return trees\n",
    "\n",
    "# Verificação de Erros\n",
    "trees = train_ensemble(X,y,10)\n",
    "assert type(trees) == list, 'A função deve retornar uma lista contendo as árvores treinadas'\n",
    "assert len(train_ensemble(X,y,9)) == 9 and len(train_ensemble(X,y,12)) == 12, 'A lista retornada deve ter exatamente n_trees elementos'\n",
    "assert np.all([type(tree) == DecisionTreeClassifier for tree in trees]), 'Todos os elementos da lista retornada devem ser uma árvore de decisão'\n",
    "assert np.all(['tree_' in tree.__dict__ for tree in trees]), 'As árvores de decisão retornadas devem estar treinadas'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para fazermos novas predições, devemos calcular a média dos valores preditos por cada árvore individual:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ensemble_predict(trees, x):\n",
    "    preds = None\n",
    "    y = []\n",
    "    ######################################################################################\n",
    "    # 5. Use a ensemble trees para fazer predições nos dados x\n",
    "    # Ao final, a variável preds deve conter as predições da ensemble\n",
    "    \n",
    "    preds = (np.sum([tree.predict(x) for tree in trees], 0))/len(trees)\n",
    "    \n",
    "    ######################################################################################\n",
    "    return preds\n",
    "\n",
    "trees = train_ensemble(X,y,10)\n",
    "assert ensemble_predict(trees, X).shape == y.shape, 'A array retornada pela função deve ter o mesmo número de elementos que a variável y correspondente'\n",
    "assert np.all((ensemble_predict(trees, X))*len(trees) == np.sum([tree.predict(X) for tree in trees], 0)), 'A predição feita pela ensemble deve ser a média das predições individuais'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Por fim, implemente uma função para medir a acurácia da ensemble:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ensemble_accuracy(trees, x_test, y_test):\n",
    "    acc = None\n",
    "    \n",
    "    ######################################################################################\n",
    "    # 6. Calcule a acurácia da ensemble trees no conjunto de teste\n",
    "    # Ao final, a variável acc deve conter o valor da acurácia\n",
    "    #\n",
    "    # OBS: Lembre-se que os valores retornados pela função ensemble_predict são probabilidades entre 0 ou 1.\n",
    "    #      Por isso, pode ser necessário usar uma função como np.round para transformar essas predições em uma variável binária\n",
    "    \n",
    "    y_pred = np.round(ensemble_predict(trees, x_test))\n",
    "    acc = accuracy_score(y_test, y_pred)\n",
    "    \n",
    "    ######################################################################################    \n",
    "    return acc\n",
    "\n",
    "# Verificação de Erros\n",
    "assert ensemble_accuracy([DecisionTreeClassifier(max_depth = None).fit(X, y)] * 10, X, y) == 1.0, 'Erro na função de acurácia'\n",
    "assert ensemble_accuracy([DecisionTreeClassifier(max_depth = 1).fit(X, y)] * 10, X, y) < 1.0, 'Erro na função de acurácia'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. K-Fold Cross Validation\n",
    "\n",
    "Devido ao dataset pequeno, usaremos K-Fold Cross Validation em vez de um simples train test split, para tornar as estimativas de performance mais confiáveis.\n",
    "\n",
    "Preencha abaixo o código para finalizarmos o treinamento de uma ensemble de árvores de decisão:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se tudo correr bem, o resultado da ensemble será consideravelmente melhor que as árvores individuais, por 5-10%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Acurácia média das árvores de decisão individuais: 74.26%\n",
      "Acurácia média da ensemble inteira: 80.20%\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "\n",
    "# Cria uma divisão KFold com 3 splits\n",
    "kf = KFold(n_splits = 3, shuffle = True, random_state = 2)\n",
    "\n",
    "# Variável com as acurácias do modelo em cada split\n",
    "ensemble_accs = []\n",
    "tree_accs = []\n",
    "\n",
    "for train_indices, test_indices in kf.split(X,y):\n",
    "    \n",
    "    # Define os conjuntos de treino e teste do split atual\n",
    "    x_train = X[train_indices]\n",
    "    x_test = X[test_indices]\n",
    "    y_train = y[train_indices]\n",
    "    y_test = y[test_indices]\n",
    "    \n",
    "    ensemble_acc = None\n",
    "    ######################################################################################\n",
    "    # 7. Treine uma ensemble nos dados de treino e a avalie nos dados de teste\n",
    "    # Ao final, a variável ensemble_acc deve conter o valor da acurácia no split atual\n",
    "    # Experimente com o número de árvores na ensemble (argumento n_trees da função train_ensemble) para conseguir os melhores resultados\n",
    "    \n",
    "    trees = train_ensemble(x_train, y_train, n_trees = 5)\n",
    "    ensemble_acc = ensemble_accuracy(trees, x_test, y_test)\n",
    "    \n",
    "    ######################################################################################\n",
    "    \n",
    "    ensemble_accs.append(ensemble_acc)\n",
    "    tree_accs.append(np.mean([tree_accuracy(tree, x_test, y_test) for tree in trees]))\n",
    "\n",
    "    \n",
    "print('Acurácia média das árvores de decisão individuais: %.2lf%%' % (100 * np.mean(tree_accs)))\n",
    "print('Acurácia média da ensemble inteira: %.2lf%%' % (100 * np.mean(ensemble_accs)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
